{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xBYxoFwMF8R",
        "outputId": "0b9922ae-7ffe-410f-cea2-9579cfe8928b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "1.25.2\n",
            "3.2.1\n",
            "0.13.1\n"
          ]
        }
      ],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import networkx as nx\n",
        "from networkx.algorithms import community\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['PYTHONWARNINGS'] = \"ignore\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data_dir = \"./data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "print(tf.__version__)\n",
        "print(np.__version__)\n",
        "print(nx.__version__)\n",
        "print(sns.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwhCPJqWppBh"
      },
      "source": [
        "## Utils module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9ZiiQzaLM2IY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse.linalg import eigsh\n",
        "import sys\n",
        "#from collections.abc import Mapping\n",
        "#from networkx.exception import NetworkXAlgorithmError\n",
        "import networkx as nx\n",
        "from networkx.utils import not_implemented_for\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    mask = np.zeros(l, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    return mask\n",
        "\n",
        "\n",
        "def load_data(dataset_fv):\n",
        "    \"\"\"\n",
        "    Loads input data feature varients from data directory\n",
        "\n",
        "    ind.dataset_fv.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_fv.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_fv.allx => the feature vectors of both labeled and unlabeled training instances\n",
        "        (a superset of ind.dataset_fv.x) as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_fv.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
        "    ind.dataset_fv.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
        "    ind.dataset_fv.ally => the labels for instances in ind.dataset_fv.allx as numpy.ndarray object;\n",
        "    ind.dataset_fv.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict object;\n",
        "    ind.dataset_fv.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
        "\n",
        "    All objects above must be saved using python pickle module.\n",
        "\n",
        "    :param dataset_fv: Dataset name\n",
        "    :return: All data input files loaded (as well the training/test data).\n",
        "    \"\"\"\n",
        "    data_vnames = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "    objects = []\n",
        "    for i in range(len(data_vnames)):\n",
        "        with open(\"/content/ind.{}.{}\".format(dataset_fv, data_vnames[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\"/content/ind.{}.test.index\".format(dataset_fv))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    if dataset_fv == 'nell':\n",
        "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "        # Find isolated nodes, add them as zero-vecs into the right position\n",
        "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "        tx = tx_extended\n",
        "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
        "        ty = ty_extended\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "\n",
        "    labels = np.vstack((ally, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "    idx_test = test_idx_range.tolist()\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y)+500)\n",
        "\n",
        "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
        "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
        "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
        "\n",
        "    y_train = np.zeros(labels.shape)\n",
        "    y_val = np.zeros(labels.shape)\n",
        "    y_test = np.zeros(labels.shape)\n",
        "    y_train[train_mask, :] = labels[train_mask, :]\n",
        "    y_val[val_mask, :] = labels[val_mask, :]\n",
        "    y_test[test_mask, :] = labels[test_mask, :]\n",
        "\n",
        "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
        "\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)\n",
        "\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "\n",
        "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
        "    \"\"\"Construct feed dictionary.\"\"\"\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict\n",
        "\n",
        "\n",
        "def chebyshev_polynomials(adj, k):\n",
        "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
        "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
        "\n",
        "    adj_normalized = normalize_adj(adj)\n",
        "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
        "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
        "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
        "\n",
        "    t_k = list()\n",
        "    t_k.append(sp.eye(adj.shape[0]))\n",
        "    t_k.append(scaled_laplacian)\n",
        "\n",
        "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
        "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
        "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
        "\n",
        "    for i in range(2, k+1):\n",
        "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
        "\n",
        "    return sparse_to_tuple(t_k)\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)\n",
        "\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "\n",
        "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
        "    \"\"\"Construct feed dictionary.\"\"\"\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict\n",
        "\n",
        "\n",
        "def chebyshev_polynomials(adj, k):\n",
        "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
        "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
        "\n",
        "    adj_normalized = normalize_adj(adj)\n",
        "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
        "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
        "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
        "\n",
        "    t_k = list()\n",
        "    t_k.append(sp.eye(adj.shape[0]))\n",
        "    t_k.append(scaled_laplacian)\n",
        "\n",
        "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
        "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
        "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
        "\n",
        "    for i in range(2, k+1):\n",
        "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
        "\n",
        "    return sparse_to_tuple(t_k)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymakyNY2q243"
      },
      "source": [
        "##Inits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R-101ouBq6hm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def uniform(shape, scale=0.05, name=None):\n",
        "    \"\"\"Uniform init.\"\"\"\n",
        "    initial = tf.Variable(tf.random.uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32))\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def glorot(shape, name=None):\n",
        "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
        "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
        "    initial = tf.Variable(tf.random.uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32))\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def zeros(shape, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.zeros(shape, dtype=tf.float64)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "\n",
        "def ones(shape, name=None):\n",
        "    \"\"\"All ones.\"\"\"\n",
        "    initial = tf.ones(shape, dtype=tf.float64)\n",
        "    return tf.Variable(initial, name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR2qCVPvqp7j"
      },
      "source": [
        "## layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4DRsVhL6qs_h"
      },
      "outputs": [],
      "source": [
        "#from inits import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "flags = tf.compat.v1.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "\n",
        "def sparse_dropout(x, keep_prob, noise_shape):\n",
        "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "\n",
        "class Layer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    Implementation inspired by keras (http://keras.io).\n",
        "\n",
        "    # Properties\n",
        "        name: String, defines the variable scope of the layer.\n",
        "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
        "\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "        _log_vars(): Log all variables\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.sparse_inputs = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            if self.logging and not self.sparse_inputs:\n",
        "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
        "            outputs = self._call(inputs)\n",
        "            if self.logging:\n",
        "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
        "            return outputs\n",
        "\n",
        "    def _log_vars(self):\n",
        "        for var in self.vars:\n",
        "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    \"\"\"Dense layer.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n",
        "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
        "        super(Dense, self).__init__(**kwargs)\n",
        "\n",
        "        if dropout:\n",
        "            self.dropout = placeholders['dropout']\n",
        "        else:\n",
        "            self.dropout = 0.\n",
        "\n",
        "        self.act = act\n",
        "        self.sparse_inputs = sparse_inputs\n",
        "        self.featureless = featureless\n",
        "        self.bias = bias\n",
        "\n",
        "        # helper variable for sparse dropout\n",
        "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
        "\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
        "                                          name='weights')\n",
        "            if self.bias:\n",
        "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
        "\n",
        "        if self.logging:\n",
        "            self._log_vars()\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "\n",
        "        # dropout\n",
        "        if self.sparse_inputs:\n",
        "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
        "        else:\n",
        "            x = tf.nn.dropout(x, 1-self.dropout)\n",
        "\n",
        "        # transform\n",
        "        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
        "\n",
        "        # bias\n",
        "        if self.bias:\n",
        "            output += self.vars['bias']\n",
        "\n",
        "        return self.act(output)\n",
        "\n",
        "\n",
        "class GraphConv(Layer):\n",
        "    \"\"\"Graph convolution layer.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
        "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
        "                 featureless=False, **kwargs):\n",
        "        super(GraphConv, self).__init__(**kwargs)\n",
        "\n",
        "        if dropout:\n",
        "            self.dropout = placeholders['dropout']\n",
        "        else:\n",
        "            self.dropout = 0.\n",
        "\n",
        "        self.act = act\n",
        "        self.support = placeholders['support']\n",
        "        self.sparse_inputs = sparse_inputs\n",
        "        self.featureless = featureless\n",
        "        self.bias = bias\n",
        "\n",
        "        # helper variable for sparse dropout\n",
        "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
        "\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            for i in range(len(self.support)):\n",
        "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
        "                                                        name='weights_' + str(i))\n",
        "            if self.bias:\n",
        "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
        "\n",
        "        if self.logging:\n",
        "            self._log_vars()\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "\n",
        "        # dropout\n",
        "        if self.sparse_inputs:\n",
        "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
        "        else:\n",
        "            x = tf.nn.dropout(x, 1-self.dropout)\n",
        "\n",
        "        # convolve\n",
        "        supports = list()\n",
        "        for i in range(len(self.support)):\n",
        "            if not self.featureless:\n",
        "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
        "                              sparse=self.sparse_inputs)\n",
        "            else:\n",
        "                pre_sup = self.vars['weights_' + str(i)]\n",
        "            support = dot(self.support[i], pre_sup, sparse=True)\n",
        "            supports.append(support)\n",
        "        output = tf.add_n(supports)\n",
        "\n",
        "        # bias\n",
        "        if self.bias:\n",
        "            output += self.vars['bias']\n",
        "\n",
        "        return self.act(output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
        "        e = self._prepare_attentional_mechanism_input(Wh)\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, Wh)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def _prepare_attentional_mechanism_input(self, Wh):\n",
        "        # Wh.shape (N, out_feature)\n",
        "        # self.a.shape (2 * out_feature, 1)\n",
        "        # Wh1&2.shape (N, 1)\n",
        "        # e.shape (N, N)\n",
        "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
        "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
        "        # broadcast add\n",
        "        e = Wh1 + Wh2.T\n",
        "        return self.leakyrelu(e)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qeCv-KUsp3C"
      },
      "source": [
        "## Metrics setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-IGewpLWsugs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "#from metrics import *\n",
        "\n",
        "def masked_softmax_cross_entropy(preds, labels, mask):\n",
        "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def masked_accuracy(preds, labels, mask):\n",
        "    \"\"\"Accuracy with masking.\"\"\"\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
        "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    accuracy_all *= mask\n",
        "    return tf.reduce_mean(accuracy_all)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHfXYHtrqe_7"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rXIqZuNCMJdX"
      },
      "outputs": [],
      "source": [
        "#from layers import *\n",
        "#from metrics import *\n",
        "\n",
        "flags = tf.compat.v1.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "        self.placeholders = {}\n",
        "\n",
        "        self.layers = []\n",
        "        self.activations = []\n",
        "\n",
        "        self.inputs = None\n",
        "        self.outputs = None\n",
        "\n",
        "        self.loss = 0\n",
        "        self.accuracy = 0\n",
        "        self.optimizer = None\n",
        "        self.opt_op = None\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "\n",
        "        # Build sequential layer model\n",
        "        self.activations.append(self.inputs)\n",
        "        for layer in self.layers:\n",
        "            hidden = layer(self.activations[-1])\n",
        "            self.activations.append(hidden)\n",
        "        self.outputs = self.activations[-1]\n",
        "\n",
        "        # Store model variables for easy access\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "        # Build metrics\n",
        "        self._loss()\n",
        "        self._accuracy()\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.loss)\n",
        "\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "    def _loss(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _accuracy(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save(self, sess=None):\n",
        "        if not sess:\n",
        "            raise AttributeError(\"TensorFlow session not provided.\")\n",
        "        saver = tf.train.Saver(self.vars)\n",
        "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
        "        print(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "    def load(self, sess=None):\n",
        "        if not sess:\n",
        "            raise AttributeError(\"TensorFlow session not provided.\")\n",
        "        saver = tf.train.Saver(self.vars)\n",
        "        save_path = \"tmp/%s.ckpt\" % self.name\n",
        "        saver.restore(sess, save_path)\n",
        "        print(\"Model restored from file: %s\" % save_path)\n",
        "\n",
        "\n",
        "class MLP(Model):\n",
        "    def __init__(self, placeholders, input_dim, **kwargs):\n",
        "        super(MLP, self).__init__(**kwargs)\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.input_dim = input_dim\n",
        "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
        "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
        "        self.placeholders = placeholders\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def _loss(self):\n",
        "        # Weight decay loss\n",
        "        for var in self.layers[0].vars.values():\n",
        "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
        "\n",
        "        # Cross entropy error\n",
        "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
        "                                                  self.placeholders['labels_mask'])\n",
        "\n",
        "    def _accuracy(self):\n",
        "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
        "                                        self.placeholders['labels_mask'])\n",
        "\n",
        "    def _build(self):\n",
        "        self.layers.append(Dense(input_dim=self.input_dim,\n",
        "                                 output_dim=FLAGS.hidden1,\n",
        "                                 placeholders=self.placeholders,\n",
        "                                 act=tf.nn.relu,\n",
        "                                 dropout=True,\n",
        "                                 sparse_inputs=True,\n",
        "                                 logging=self.logging))\n",
        "\n",
        "        self.layers.append(Dense(input_dim=FLAGS.hidden1,\n",
        "                                 output_dim=self.output_dim,\n",
        "                                 placeholders=self.placeholders,\n",
        "                                 act=lambda x: x,\n",
        "                                 dropout=True,\n",
        "                                 logging=self.logging))\n",
        "\n",
        "    def predict(self):\n",
        "        return tf.nn.softmax(self.outputs)\n",
        "\n",
        "\n",
        "class GCN(Model):\n",
        "    def __init__(self, placeholders, input_dim, **kwargs):\n",
        "        super(GCN, self).__init__(**kwargs)\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.input_dim = input_dim\n",
        "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
        "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
        "        self.placeholders = placeholders\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def _loss(self):\n",
        "        # Weight decay loss\n",
        "        for var in self.layers[0].vars.values():\n",
        "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
        "\n",
        "        # Cross entropy error\n",
        "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
        "                                                  self.placeholders['labels_mask'])\n",
        "\n",
        "    def _accuracy(self):\n",
        "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
        "                                        self.placeholders['labels_mask'])\n",
        "\n",
        "    def _build(self):\n",
        "\n",
        "        self.layers.append(GraphConv(input_dim=self.input_dim,\n",
        "                                            output_dim=FLAGS.hidden1,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=tf.nn.relu,\n",
        "                                            dropout=True,\n",
        "                                            sparse_inputs=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "        self.layers.append(GraphConv(input_dim=FLAGS.hidden1,\n",
        "                                            output_dim=self.output_dim,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=lambda x: x,\n",
        "                                            dropout=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "    def predict(self):\n",
        "        return tf.nn.softmax(self.outputs)\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, labels,nclass, dropout, alpha, nheads, placeholders, input_dim, logging=False):\n",
        "        \"\"\"Dense version of GAT.\"\"\"\n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.elu(self.out_att(x, adj))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def _accuracy(self):\n",
        "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
        "                                        self.placeholders['labels_mask'])\n",
        "\n",
        "    def _build(self):\n",
        "\n",
        "        self.layers.append(GraphAttentionLayer(input_dim=self.input_dim,\n",
        "                                            output_dim=FLAGS.hidden1,\n",
        "\n",
        "                                               nheads=flags.nb_heads,\n",
        "                                               alpha=flags.alpha,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=tf.nn.relu,\n",
        "                                               weight_decay=flags.weight_decay,\n",
        "                                            dropout=True,\n",
        "                                            sparse_inputs=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "        self.layers.append(GraphAttentionLayer(input_dim=FLAGS.hidden1,\n",
        "                                            output_dim=self.output_dim,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=lambda x: x,\n",
        "                                            dropout=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "    def predict(self):\n",
        "        return tf.nn.softmax(self.outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BUNcTRfhv_A",
        "outputId": "1527b3b8-f30b-499e-f9c2-6717527df79d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-8fcea97eca8e>:48: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
            "  objects.append(pkl.load(f, encoding='latin1'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Chebyshev polynomials up to order 3...\n",
            "Epoch: 0001 train_loss= 2.18495 train_acc= 0.13571 val_loss= 2.17475 val_acc= 0.16000 time= 0.39791\n",
            "Epoch: 0002 train_loss= 2.16915 train_acc= 0.18571 val_loss= 2.16627 val_acc= 0.18400 time= 0.10831\n",
            "Epoch: 0003 train_loss= 2.15519 train_acc= 0.21429 val_loss= 2.15804 val_acc= 0.21400 time= 0.11196\n",
            "Epoch: 0004 train_loss= 2.13271 train_acc= 0.30000 val_loss= 2.15007 val_acc= 0.24200 time= 0.10428\n",
            "Epoch: 0005 train_loss= 2.11814 train_acc= 0.35714 val_loss= 2.14222 val_acc= 0.28200 time= 0.12606\n",
            "Epoch: 0006 train_loss= 2.10642 train_acc= 0.43571 val_loss= 2.13442 val_acc= 0.31000 time= 0.12604\n",
            "Epoch: 0007 train_loss= 2.08951 train_acc= 0.52143 val_loss= 2.12661 val_acc= 0.35400 time= 0.11827\n",
            "Epoch: 0008 train_loss= 2.06852 train_acc= 0.62857 val_loss= 2.11876 val_acc= 0.38600 time= 0.11518\n",
            "Epoch: 0009 train_loss= 2.06005 train_acc= 0.62143 val_loss= 2.11084 val_acc= 0.42400 time= 0.12162\n",
            "Epoch: 0010 train_loss= 2.04293 train_acc= 0.75000 val_loss= 2.10284 val_acc= 0.47000 time= 0.11846\n",
            "Epoch: 0011 train_loss= 2.02801 train_acc= 0.75714 val_loss= 2.09470 val_acc= 0.51200 time= 0.11626\n",
            "Epoch: 0012 train_loss= 2.00492 train_acc= 0.81429 val_loss= 2.08635 val_acc= 0.53000 time= 0.10725\n",
            "Epoch: 0013 train_loss= 1.99020 train_acc= 0.88571 val_loss= 2.07788 val_acc= 0.55000 time= 0.12802\n",
            "Epoch: 0014 train_loss= 1.97424 train_acc= 0.85000 val_loss= 2.06927 val_acc= 0.57600 time= 0.12875\n",
            "Epoch: 0015 train_loss= 1.95063 train_acc= 0.90000 val_loss= 2.06050 val_acc= 0.59000 time= 0.11456\n",
            "Epoch: 0016 train_loss= 1.92699 train_acc= 0.92857 val_loss= 2.05152 val_acc= 0.59800 time= 0.10973\n",
            "Epoch: 0017 train_loss= 1.91840 train_acc= 0.95000 val_loss= 2.04240 val_acc= 0.62200 time= 0.10644\n",
            "Epoch: 0018 train_loss= 1.89558 train_acc= 0.95000 val_loss= 2.03313 val_acc= 0.63400 time= 0.10862\n",
            "Epoch: 0019 train_loss= 1.87920 train_acc= 0.92857 val_loss= 2.02370 val_acc= 0.64400 time= 0.12255\n",
            "Epoch: 0020 train_loss= 1.85495 train_acc= 0.95714 val_loss= 2.01412 val_acc= 0.65000 time= 0.15899\n",
            "Epoch: 0021 train_loss= 1.83378 train_acc= 0.96429 val_loss= 2.00444 val_acc= 0.65800 time= 0.17463\n",
            "Epoch: 0022 train_loss= 1.81472 train_acc= 0.94286 val_loss= 1.99463 val_acc= 0.66200 time= 0.18102\n",
            "Epoch: 0023 train_loss= 1.78646 train_acc= 0.97143 val_loss= 1.98473 val_acc= 0.67600 time= 0.17571\n",
            "Epoch: 0024 train_loss= 1.76277 train_acc= 0.97857 val_loss= 1.97474 val_acc= 0.68400 time= 0.17728\n",
            "Epoch: 0025 train_loss= 1.73814 train_acc= 0.98571 val_loss= 1.96467 val_acc= 0.68200 time= 0.18561\n",
            "Epoch: 0026 train_loss= 1.73016 train_acc= 0.97857 val_loss= 1.95465 val_acc= 0.68800 time= 0.17442\n",
            "Epoch: 0027 train_loss= 1.69685 train_acc= 1.00000 val_loss= 1.94462 val_acc= 0.69400 time= 0.17623\n",
            "Epoch: 0028 train_loss= 1.67911 train_acc= 0.98571 val_loss= 1.93465 val_acc= 0.69400 time= 0.17876\n",
            "Epoch: 0029 train_loss= 1.64460 train_acc= 0.97143 val_loss= 1.92463 val_acc= 0.69400 time= 0.17070\n",
            "Epoch: 0030 train_loss= 1.63056 train_acc= 0.97857 val_loss= 1.91462 val_acc= 0.70000 time= 0.17494\n",
            "Epoch: 0031 train_loss= 1.59057 train_acc= 0.98571 val_loss= 1.90463 val_acc= 0.71000 time= 0.18476\n",
            "Epoch: 0032 train_loss= 1.58185 train_acc= 0.99286 val_loss= 1.89467 val_acc= 0.71000 time= 0.17795\n",
            "Epoch: 0033 train_loss= 1.56881 train_acc= 0.97857 val_loss= 1.88467 val_acc= 0.71400 time= 0.18315\n",
            "Epoch: 0034 train_loss= 1.54411 train_acc= 0.99286 val_loss= 1.87463 val_acc= 0.71800 time= 0.18984\n",
            "Epoch: 0035 train_loss= 1.51561 train_acc= 0.97857 val_loss= 1.86460 val_acc= 0.71800 time= 0.11007\n",
            "Epoch: 0036 train_loss= 1.49640 train_acc= 0.99286 val_loss= 1.85455 val_acc= 0.72400 time= 0.10569\n",
            "Epoch: 0037 train_loss= 1.45468 train_acc= 0.98571 val_loss= 1.84456 val_acc= 0.72800 time= 0.11160\n",
            "Epoch: 0038 train_loss= 1.42114 train_acc= 0.99286 val_loss= 1.83452 val_acc= 0.73400 time= 0.10581\n",
            "Epoch: 0039 train_loss= 1.41581 train_acc= 0.99286 val_loss= 1.82446 val_acc= 0.73200 time= 0.11033\n",
            "Epoch: 0040 train_loss= 1.37456 train_acc= 1.00000 val_loss= 1.81437 val_acc= 0.73600 time= 0.10591\n",
            "Epoch: 0041 train_loss= 1.37408 train_acc= 0.99286 val_loss= 1.80423 val_acc= 0.74000 time= 0.10845\n",
            "Epoch: 0042 train_loss= 1.33306 train_acc= 0.99286 val_loss= 1.79410 val_acc= 0.74200 time= 0.11080\n",
            "Epoch: 0043 train_loss= 1.32454 train_acc= 0.98571 val_loss= 1.78401 val_acc= 0.74200 time= 0.12353\n",
            "Epoch: 0044 train_loss= 1.30910 train_acc= 0.98571 val_loss= 1.77395 val_acc= 0.75000 time= 0.10985\n",
            "Epoch: 0045 train_loss= 1.27274 train_acc= 1.00000 val_loss= 1.76377 val_acc= 0.75200 time= 0.10462\n",
            "Epoch: 0046 train_loss= 1.23737 train_acc= 0.99286 val_loss= 1.75363 val_acc= 0.75000 time= 0.10557\n",
            "Epoch: 0047 train_loss= 1.22681 train_acc= 0.98571 val_loss= 1.74340 val_acc= 0.75000 time= 0.11462\n",
            "Epoch: 0048 train_loss= 1.21410 train_acc= 0.98571 val_loss= 1.73311 val_acc= 0.75200 time= 0.11102\n",
            "Epoch: 0049 train_loss= 1.17780 train_acc= 0.99286 val_loss= 1.72276 val_acc= 0.75200 time= 0.10878\n",
            "Epoch: 0050 train_loss= 1.15600 train_acc= 0.99286 val_loss= 1.71252 val_acc= 0.75600 time= 0.10525\n",
            "Epoch: 0051 train_loss= 1.13592 train_acc= 1.00000 val_loss= 1.70226 val_acc= 0.75800 time= 0.10908\n",
            "Epoch: 0052 train_loss= 1.10275 train_acc= 1.00000 val_loss= 1.69194 val_acc= 0.75800 time= 0.11955\n",
            "Epoch: 0053 train_loss= 1.06771 train_acc= 1.00000 val_loss= 1.68157 val_acc= 0.75800 time= 0.11164\n",
            "Epoch: 0054 train_loss= 1.07210 train_acc= 0.98571 val_loss= 1.67140 val_acc= 0.75800 time= 0.10790\n",
            "Epoch: 0055 train_loss= 1.05504 train_acc= 0.99286 val_loss= 1.66134 val_acc= 0.76000 time= 0.10695\n",
            "Epoch: 0056 train_loss= 1.02639 train_acc= 0.99286 val_loss= 1.65142 val_acc= 0.76200 time= 0.10339\n",
            "Epoch: 0057 train_loss= 1.00955 train_acc= 1.00000 val_loss= 1.64160 val_acc= 0.76400 time= 0.10748\n",
            "Epoch: 0058 train_loss= 1.01577 train_acc= 0.99286 val_loss= 1.63175 val_acc= 0.76600 time= 0.10311\n",
            "Epoch: 0059 train_loss= 0.97191 train_acc= 0.98571 val_loss= 1.62202 val_acc= 0.76800 time= 0.10857\n",
            "Epoch: 0060 train_loss= 0.97028 train_acc= 1.00000 val_loss= 1.61236 val_acc= 0.76800 time= 0.10436\n",
            "Epoch: 0061 train_loss= 0.91640 train_acc= 1.00000 val_loss= 1.60267 val_acc= 0.76800 time= 0.11668\n",
            "Epoch: 0062 train_loss= 0.90733 train_acc= 1.00000 val_loss= 1.59304 val_acc= 0.76800 time= 0.11584\n",
            "Epoch: 0063 train_loss= 0.90433 train_acc= 0.99286 val_loss= 1.58353 val_acc= 0.77000 time= 0.10770\n",
            "Epoch: 0064 train_loss= 0.89865 train_acc= 0.98571 val_loss= 1.57411 val_acc= 0.77000 time= 0.10490\n",
            "Epoch: 0065 train_loss= 0.88612 train_acc= 1.00000 val_loss= 1.56474 val_acc= 0.77000 time= 0.12012\n",
            "Epoch: 0066 train_loss= 0.84862 train_acc= 1.00000 val_loss= 1.55551 val_acc= 0.77000 time= 0.10472\n",
            "Epoch: 0067 train_loss= 0.84759 train_acc= 0.98571 val_loss= 1.54645 val_acc= 0.77000 time= 0.11020\n",
            "Epoch: 0068 train_loss= 0.80596 train_acc= 1.00000 val_loss= 1.53742 val_acc= 0.77000 time= 0.11008\n",
            "Epoch: 0069 train_loss= 0.82536 train_acc= 1.00000 val_loss= 1.52846 val_acc= 0.77400 time= 0.11749\n",
            "Epoch: 0070 train_loss= 0.80563 train_acc= 0.99286 val_loss= 1.51978 val_acc= 0.77600 time= 0.11682\n",
            "Epoch: 0071 train_loss= 0.79078 train_acc= 1.00000 val_loss= 1.51144 val_acc= 0.77200 time= 0.14090\n",
            "Epoch: 0072 train_loss= 0.74948 train_acc= 1.00000 val_loss= 1.50334 val_acc= 0.77200 time= 0.12057\n",
            "Epoch: 0073 train_loss= 0.76406 train_acc= 1.00000 val_loss= 1.49543 val_acc= 0.77200 time= 0.11554\n",
            "Epoch: 0074 train_loss= 0.73861 train_acc= 1.00000 val_loss= 1.48765 val_acc= 0.77200 time= 0.11626\n",
            "Epoch: 0075 train_loss= 0.72456 train_acc= 1.00000 val_loss= 1.47986 val_acc= 0.77200 time= 0.12333\n",
            "Epoch: 0076 train_loss= 0.71752 train_acc= 1.00000 val_loss= 1.47217 val_acc= 0.77400 time= 0.11303\n",
            "Epoch: 0077 train_loss= 0.69985 train_acc= 1.00000 val_loss= 1.46443 val_acc= 0.77200 time= 0.12024\n",
            "Epoch: 0078 train_loss= 0.69987 train_acc= 1.00000 val_loss= 1.45690 val_acc= 0.77200 time= 0.10818\n",
            "Epoch: 0079 train_loss= 0.70031 train_acc= 1.00000 val_loss= 1.44946 val_acc= 0.77200 time= 0.11796\n",
            "Epoch: 0080 train_loss= 0.68052 train_acc= 1.00000 val_loss= 1.44239 val_acc= 0.77600 time= 0.11185\n",
            "Epoch: 0081 train_loss= 0.65783 train_acc= 1.00000 val_loss= 1.43566 val_acc= 0.77600 time= 0.10972\n",
            "Epoch: 0082 train_loss= 0.67626 train_acc= 0.99286 val_loss= 1.42916 val_acc= 0.77600 time= 0.10230\n",
            "Epoch: 0083 train_loss= 0.67415 train_acc= 1.00000 val_loss= 1.42273 val_acc= 0.77600 time= 0.11178\n",
            "Epoch: 0084 train_loss= 0.63337 train_acc= 1.00000 val_loss= 1.41650 val_acc= 0.77600 time= 0.11155\n",
            "Epoch: 0085 train_loss= 0.64785 train_acc= 1.00000 val_loss= 1.41056 val_acc= 0.77600 time= 0.11074\n",
            "Epoch: 0086 train_loss= 0.61816 train_acc= 1.00000 val_loss= 1.40464 val_acc= 0.77600 time= 0.11137\n",
            "Epoch: 0087 train_loss= 0.61062 train_acc= 0.99286 val_loss= 1.39872 val_acc= 0.77600 time= 0.12415\n",
            "Epoch: 0088 train_loss= 0.60795 train_acc= 1.00000 val_loss= 1.39273 val_acc= 0.77600 time= 0.12390\n",
            "Epoch: 0089 train_loss= 0.60653 train_acc= 0.99286 val_loss= 1.38704 val_acc= 0.77600 time= 0.12809\n",
            "Epoch: 0090 train_loss= 0.56644 train_acc= 1.00000 val_loss= 1.38143 val_acc= 0.77600 time= 0.11229\n",
            "Epoch: 0091 train_loss= 0.61108 train_acc= 0.99286 val_loss= 1.37602 val_acc= 0.77600 time= 0.12262\n",
            "Epoch: 0092 train_loss= 0.58683 train_acc= 1.00000 val_loss= 1.37072 val_acc= 0.77400 time= 0.12142\n",
            "Epoch: 0093 train_loss= 0.57817 train_acc= 1.00000 val_loss= 1.36560 val_acc= 0.77400 time= 0.16727\n",
            "Epoch: 0094 train_loss= 0.55665 train_acc= 1.00000 val_loss= 1.36084 val_acc= 0.77400 time= 0.17479\n",
            "Epoch: 0095 train_loss= 0.55375 train_acc= 1.00000 val_loss= 1.35616 val_acc= 0.77600 time= 0.16942\n",
            "Epoch: 0096 train_loss= 0.56157 train_acc= 1.00000 val_loss= 1.35149 val_acc= 0.77400 time= 0.18826\n",
            "Epoch: 0097 train_loss= 0.54037 train_acc= 1.00000 val_loss= 1.34688 val_acc= 0.77400 time= 0.18554\n",
            "Epoch: 0098 train_loss= 0.52272 train_acc= 1.00000 val_loss= 1.34256 val_acc= 0.77400 time= 0.17184\n",
            "Epoch: 0099 train_loss= 0.54281 train_acc= 1.00000 val_loss= 1.33819 val_acc= 0.77400 time= 0.18442\n",
            "Epoch: 0100 train_loss= 0.51027 train_acc= 1.00000 val_loss= 1.33377 val_acc= 0.77400 time= 0.16801\n",
            "Optimization Finished!\n",
            "Test set results: cost= 1.31581 accuracy= 0.79500 time= 0.07536\n"
          ]
        }
      ],
      "source": [
        "from absl import flags\n",
        "from absl.flags import FLAGS\n",
        "from enum import Flag\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import pydot\n",
        "import graphviz\n",
        "import os\n",
        "import sys\n",
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "#from inits import *\n",
        "from scipy.sparse.linalg import eigsh\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "# Settings\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()\n",
        "    keys_list = [keys for keys in flags_dict]\n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.compat.v1.flags.FLAGS)\n",
        "\n",
        "\n",
        "flags = tf.compat.v1.app.flags\n",
        "FLAGS = tf.compat.v1.app.flags.FLAGS\n",
        "FLAGS = flags.FLAGS\n",
        "tf.compat.v1.flags.DEFINE_string('f','','')\n",
        "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
        "flags.DEFINE_string('model', 'gcn_cheby', 'Model string.')  # 'gcn', gat, 'gcn_cheby', 'dense'\n",
        "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
        "flags.DEFINE_integer('epochs', 100, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
        "flags.DEFINE_integer('early_stopping', 100, 'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
        "flags.DEFINE_float('imbalance_ratio', 0.2, 'Imbalance Ratio')\n",
        "flags.DEFINE_float('lambda', 0, 'parameter sensitivity')\n",
        "flags.DEFINE_integer('attention', 8, 'number of attention heads')\n",
        "\n",
        "\n",
        "# Load data\n",
        "sys.argv = sys.argv[:1]\n",
        "\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
        "\n",
        "# Some preprocessing\n",
        "features = preprocess_features(features)\n",
        "if FLAGS.model == 'gcn':\n",
        "    support = [preprocess_adj(adj)]\n",
        "    num_supports = 1\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'gcn_cheby':\n",
        "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
        "    num_supports = 1 + FLAGS.max_degree\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'gat':\n",
        "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
        "    num_supports = 1 + FLAGS.max_degree\n",
        "    model_func = GAT\n",
        "elif FLAGS.model == 'dense':\n",
        "    support = [preprocess_adj(adj)]  # Not used\n",
        "    num_supports = 1\n",
        "    model_func = MLP\n",
        "else:\n",
        "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
        "\n",
        "# Define placeholders\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#model_func = tf.Variable(tf.random.uniform(shape=[None, model_func._num_states]), dtype=tf.float32)\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
        "}\n",
        "\n",
        "# Create model\n",
        "\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
        "\n",
        "\n",
        "# Init variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "val_acc = []\n",
        "\n",
        "train_acc = []\n",
        "# Train model\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    # Training step\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
        "    train_acc.append(outs[1])\n",
        "\n",
        "    # Validation\n",
        "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
        "    cost_val.append(cost)\n",
        "    val_acc.append(acc)\n",
        "\n",
        "\n",
        "\n",
        "    # Print results\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
        "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "\n",
        "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
        "        print(\"Early stopping...\")\n",
        "        break\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "test_acc=[]\n",
        "# Testing\n",
        "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
        "\n",
        "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}